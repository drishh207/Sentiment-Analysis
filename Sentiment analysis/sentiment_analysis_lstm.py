# -*- coding: utf-8 -*-
"""Sentiment analysis 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VvIrmDc6Ck8Mfu44iVJVnQvytJdSx9Y9
"""

# Import Libraries
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os
import re
import string
from sklearn.feature_extraction.text import CountVectorizer

from google.colab import drive
drive.mount('/content/drive')

import glob

path = r'drive/MyDrive/Sentiment analysis/' # use your path
all_files = glob.glob(path + "/*.CSV")
print(all_files)
li = []

for filename in all_files:
    df = pd.read_csv(filename, usecols = ['text'], index_col=None, header=0, nrows = 8000,encoding = "ISO-8859-1")
    li.append(df)


df = pd.read_csv("drive/MyDrive/Sentiment analysis/covid19_tweets.csv",usecols = ['text'],encoding = "ISO-8859-1",nrows = 178687)
li.append(df)

frame = pd.concat(li, axis=0, ignore_index=True)
tweets = pd.DataFrame(frame)
print(tweets)
index = tweets.index
noOfTweet = len(index)

#cleaning the tweets
def remove_pattern(input_txt, pattern):
  r = re.findall(pattern, str(input_txt))
  for i in r:
    input_txt = re.sub(i, '', str(input_txt))        
  return input_txt

def clean_tweets(tweet):
  #remove twitter Return handles (RT @xxx:)
  tweet = np.vectorize(remove_pattern)(tweet, "RT @[\w]*:")
    
  #remove twitter handles (@xxx)
  tweet = np.vectorize(remove_pattern)(tweet, "@[\w]*")
    
  #remove URL links (httpxxx)
  tweet = np.vectorize(remove_pattern)(tweet, "https?://[A-Za-z0-9./]*")
    
  #remove special characters, numbers, punctuations (except for #)
  tweet = np.core.defchararray.replace(tweet, "[^a-zA-Z]", " ")
    
  return tweet

df = clean_tweets(tweets)
print(df[0:10])

!pip install vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

scores = []
# Declare variables for scores
positive = 0
negative = 0
neutral = 0
positive_list = []
negative_list = []
neutral_list = []
labels = []

for i in range(df.shape[0]):
  compound = analyzer.polarity_scores(df[i])["compound"]
  pos = analyzer.polarity_scores(df[i])["pos"]
  neu = analyzer.polarity_scores(df[i])["neu"]
  neg = analyzer.polarity_scores(df[i])["neg"]
  if(neg > pos):
    negative_list.append(df[i])
    status = "Negative"
    negative+=1
  elif (pos > neg):
    positive_list.append(df[i])
    status = "Positive"
    positive+=1
  elif (pos == neg):
    neutral_list.append(df[i])
    status = "Neutral"
    neutral+=1
  scores.append({"Compound": compound, "Positive": pos, "Negative": neg, "Neutral": neu, "Status": status})
  labels.append(status)

#print(scores[1:10])
df1 = pd.DataFrame(df)
Y_train = pd.DataFrame.from_dict(scores)
df1 = df1.join(Y_train)
print(df1[50000:50020])

def percentage(part,whole):
  return 100 * float(part)/float(whole)

positive = percentage(positive, noOfTweet)
negative = percentage(negative, noOfTweet)
neutral = percentage(neutral, noOfTweet)
positive = format(positive, '.1f')
negative = format(negative, '.1f')
neutral = format(neutral, '.1f')

#Number of Tweets (Total, Positive, Negative, Neutral)
tweet_list = pd.DataFrame(df1)
neutral_list = pd.DataFrame(neutral_list)
negative_list = pd.DataFrame(negative_list)
positive_list = pd.DataFrame(positive_list)
print("total number: ",len(tweet_list))
print("positive number: ",len(positive_list))
print("negative number: ", len(negative_list))
print("neutral number: ",len(neutral_list))

#Creating PieCart
labels = ['Positive ['+str(positive)+'%' , 'Neutral ['+str(neutral)+'%]','Negative ['+str(negative)+'%]']
sizes = [positive, neutral, negative]
colors = ['gray', 'black','brown']
patches, texts = plt.pie(sizes,colors=colors, startangle=90)
plt.style.use('default')
plt.legend(labels)
plt.title("Sentiment Analysis Result")
plt.axis('equal')
plt.show()

#positive words
HT_positive = []
def hashtag_extract(x):
    hashtags = []
    # Loop over the words in the tweet
    for i in x:
        ht = re.findall(r"#(\w+)", i)
        hashtags.append(ht)
    return hashtags

# extracting hashtags from  tweets
HT_positive = hashtag_extract(df1[0][df1["Compound"] > 0.5])
# unnesting list
HT_positive = sum(HT_positive,[])
HT_positive

HT_negative = []
def hashtag_extract(x):
    hashtags = []
    # Loop over the words in the tweet
    for i in x:
        ht = re.findall(r"#(\w+)", i)
        hashtags.append(ht)
    return hashtags
# extracting hashtags from negative tweets
# extracting hashtags from  tweets
HT_negative = hashtag_extract(df1[0][df1["Compound"] < 0.5])
# unnesting list
HT_negative= sum(HT_negative,[])
HT_negative

#most popular and trending keywords in text datasets based on the frequency of occurrence and importance.
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
def word_cloud(wd_list):
    stopwords = set(STOPWORDS)
    all_words = ' '.join([text for text in wd_list])
    wordcloud = WordCloud(
        background_color='black',
        stopwords=stopwords,
        width=1600,
        height=800,
        random_state=1,
        colormap='jet',
        max_words=80,
        max_font_size=200).generate(all_words)
    plt.figure(figsize=(12, 10))
    plt.axis('off')
    plt.imshow(wordcloud, interpolation="bilinear");
word_cloud(df1[0])

#print(df1)
from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
df_final = df1[[0,"Status"]].copy()
print(df_final)
df_final = df_final[df_final.Status != "Neutral"]
df_final[0] = df_final[0].apply(lambda x: x.lower())
df_final[0] = df_final[0].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))

print(df_final[ df_final['Status'] == 'Positive'].size)
print(df_final[df_final['Status'] == 'Negative'].size)

for idx,row in df1.iterrows():
    row[0] = row[0].replace('rt',' ')
    
max_fatures = 2000
tokenizer = Tokenizer(num_words=max_fatures, split=' ')
tokenizer.fit_on_texts(df_final[0].values)
X = tokenizer.texts_to_sequences(df_final[0].values)
X = pad_sequences(X)
print(type(X))

from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
import re

embed_dim = 128
lstm_out = 196

model = Sequential()
model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))
model.add(SpatialDropout1D(0.4))
model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(2,activation='softmax'))
model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])
print(model.summary())

Y = pd.get_dummies(df_final['Status']).values
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)
#print(X_train)
#print(Y_train)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

batch_size = 1000
model.fit(X_train, Y_train, epochs = 10, batch_size=batch_size, verbose = 2)

validation_size = 1500

X_validate = X_test[-validation_size:]
Y_validate = Y_test[-validation_size:]
X_test = X_test[:-validation_size]
Y_test = Y_test[:-validation_size]
score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)
print("score: %.2f" % (score))
print("acc: %.2f" % (acc))

pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0
for x in range(len(X_validate)):
  result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]
  if np.argmax(result) == np.argmax(Y_validate[x]):
    if np.argmax(Y_validate[x]) == 0:
      neg_correct += 1
    else:
      pos_correct += 1
       
  if np.argmax(Y_validate[x]) == 0:
    neg_cnt += 1
  else:
    pos_cnt += 1

print("pos_acc", pos_correct/pos_cnt*100, "%")
print("neg_acc", neg_correct/neg_cnt*100, "%")

twt1 = ['MFirst phase of #COVAXINVaccine was completed at PGIMS Rohtak now, Second phase of #HumanTrials of indigenously']
twt2 = ['Chief Minister @NitishKumar has ordered immediate antigen testing of the flood victims taking shelter in the governance']
twt3 = ['If your childs teacher or a other classmate gets #COVID19 your child will have to isolate for 14 days']
twt = ['Help slow the spread of #COVID19 and identify at risk cases sooner by self-reporting your symptoms daily, even if']
#vectorizing the tweet by the pre-fitted tokenizer instance
twt = tokenizer.texts_to_sequences(twt)
#padding the tweet to have exactly the same shape as 'embedding_2' input
twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)
#print(twt)
sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]
if(np.argmax(sentiment) == 0):
    print("negative")
elif (np.argmax(sentiment) == 1):
    print("positive")