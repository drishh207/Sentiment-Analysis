# -*- coding: utf-8 -*-
"""Sentiment analysis naive bayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RxluEKHPXnof6TM6I0aRP98OPTUZ6C13
"""

#Importing libraries
!pip install vaderSentiment
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os
import re
import string
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical

#loading set
from google.colab import drive
drive.mount('/content/drive')

import glob

path = r'drive/MyDrive/Sentiment analysis/' # use your path
all_files = glob.glob(path + "/*.CSV")
#print(all_files)
li = []

for filename in all_files:
    df = pd.read_csv(filename, usecols = ['text'], index_col=None, header=0, nrows = 8000,encoding = "ISO-8859-1")
    li.append(df)


df = pd.read_csv("drive/MyDrive/Sentiment analysis/covid19_tweets.csv",usecols = ['text'],encoding = "ISO-8859-1",nrows = 178687)
li.append(df)

frame = pd.concat(li, axis=0, ignore_index=True)
tweets = pd.DataFrame(frame)
print(tweets)
index = tweets.index
noOfTweet = len(index)

#cleaning the tweets
def remove_pattern(input_txt, pattern):
  r = re.findall(pattern, str(input_txt))
  for i in r:
    input_txt = re.sub(i, '', str(input_txt))        
  return input_txt

def clean_tweets(tweet):
  #remove twitter Return handles (RT @xxx:)
  tweet = np.vectorize(remove_pattern)(tweet, "RT @[\w]*:")
    
  #remove twitter handles (@xxx)
  tweet = np.vectorize(remove_pattern)(tweet, "@[\w]*")
    
  #remove URL links (httpxxx)
  tweet = np.vectorize(remove_pattern)(tweet, "https?://[A-Za-z0-9./]*")
    
  #remove special characters, numbers, punctuations (except for #)
  tweet = np.core.defchararray.replace(tweet, "[^a-zA-Z]", " ")
    
  return tweet

df = clean_tweets(tweets)
print(df[1:10])

analyzer = SentimentIntensityAnalyzer()

scores = []
# Declare variables for scores
positive = 0
negative = 0
neutral = 0
compound_list = []
positive_list = []
negative_list = []
neutral_list = []
labels = []

for i in range(df.shape[0]):
  compound = analyzer.polarity_scores(df[i])["compound"]
  pos = analyzer.polarity_scores(df[i])["pos"]
  neu = analyzer.polarity_scores(df[i])["neu"]
  neg = analyzer.polarity_scores(df[i])["neg"]
  if(neg > pos):
    negative_list.append(df[i])
    status = "Negative"
    negative+=1
  elif (pos > neg):
    positive_list.append(df[i])
    status = "Positive"
    positive+=1
  elif (pos == neg):
    neutral_list.append(df[i])
    status = "Neutral"
    neutral+=1
  scores.append({"Compound": compound, "Positive": pos, "Negative": neg, "Neutral": neu, "Status": status})
  labels.append(status)

df1 = pd.DataFrame(df)
Y_train = pd.DataFrame.from_dict(scores)
df1 = df1.join(Y_train)
df1.head(1000)

def percentage(part,whole):
  return 100 * float(part)/float(whole)

positive = percentage(positive, noOfTweet)
negative = percentage(negative, noOfTweet)
neutral = percentage(neutral, noOfTweet)
positive = format(positive, '.1f')
negative = format(negative, '.1f')
neutral = format(neutral, '.1f')

#Number of Tweets (Total, Positive, Negative, Neutral)
tweet_list = pd.DataFrame(df1)
neutral_list = pd.DataFrame(neutral_list)
negative_list = pd.DataFrame(negative_list)
positive_list = pd.DataFrame(positive_list)
print("total number:",len(tweet_list))
print("positive number:",len(positive_list))
print("negative number:", len(negative_list))
print("neutral number:",len(neutral_list))

df_final = df1[[0,"Status"]].copy()
#print(df_final)
df_final = df_final[df_final.Status != "Neutral"]
df_final[0] = df_final[0].apply(lambda x: x.lower())
df_final[0] = df_final[0].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))

print(df_final[ df_final['Status'] == 'Positive'].size)
print(df_final[df_final['Status'] == 'Negative'].size)

for idx,row in df1.iterrows():
    row[0] = row[0].replace('rt',' ')
X = df_final[0]
#print(X)

import sklearn.model_selection as model_selection
Y = df_final['Status']
#print(Y)
#print(type(Y))
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,train_size=0.80, test_size=0.20, random_state=0)
"""print(X_train)
print(Y_train)"""
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)
#print(type(X_train))
"""x_train = pd.DataFrame(X_train)
y_train = pd.DataFrame(Y_train)
x_test = pd.DataFrame(X_test)"""
from sklearn.feature_extraction.text import CountVectorizer 
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,recall_score,precision_score
from sklearn.naive_bayes import MultinomialNB

cv = CountVectorizer()
x_train = cv.fit_transform(X_train)
y_train = Y_train
x_test = cv.transform(X_test)
y_test = Y_test
#print(x_test)

nb = MultinomialNB()
nb.fit(x_train,y_train)
nb_predict=nb.predict(x_test)
#print(nb_predict)
nb_report = accuracy_score(y_test,nb_predict)
print('Accuracy:',nb_report)
#print(y_test[1:20])
#print(nb_predict[1:20])

nb_report2 = precision_score(y_test,nb_predict,pos_label='Positive',average='binary')
print('Precision:',nb_report2)

twt1 = ['MFirst phase of #COVAXINVaccine was completed at PGIMS Rohtak now, Second phase of #HumanTrials of indigenously']
twt2 = ['Chief Minister @NitishKumar has ordered immediate antigen testing of the flood victims taking shelter in the governance']
twt3 = ['Help slow the spread of #COVID19 and identify at risk cases sooner by self-reporting your symptoms daily, even if']
twt = cv.transform(twt3)
print(nb.predict(twt))